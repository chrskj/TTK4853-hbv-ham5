{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/kratzert/pangeo_lstm_example/blob/master/LSTM_for_rainfall_runoff_modelling.ipynb\n",
    "def calc_nse(obs: np.array, sim: np.array) -> float:\n",
    "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n",
    "\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: NSE value.\n",
    "    \"\"\"\n",
    "    # only consider time steps, where observations are available\n",
    "    sim = np.delete(sim, np.argwhere(obs < 0), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(obs < 0), axis=0)\n",
    "\n",
    "    # check for NaNs in observations\n",
    "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n",
    "\n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2)\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    nse_val = 1 - numerator / denominator\n",
    "\n",
    "    return nse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all input txt files\n",
    "import glob\n",
    "\n",
    "path = \"../Input files (.txt)\"\n",
    "all_files = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "df_dict = {}\n",
    "for file_path in all_files:\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    # Name is formatted `./Input files (.txt)/nve_inp_XX.txt`\n",
    "    number = int(file_path.split(\"_\")[-1].split(\".\")[0])\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        encoding=\"cp1252\",\n",
    "        skiprows=[0],\n",
    "        delimiter=r\"\\s+\",\n",
    "        parse_dates=[[\"dd.mm.yyyy\", \"hh:mm:ss\"]],\n",
    "    )\n",
    "    df = df.rename(columns={\"dd.mm.yyyy_hh:mm:ss\": \"timestamp\"})\n",
    "    df_dict[number] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All files have equal values for grC and grC.1\n",
    "for df in df_dict.values():\n",
    "    print(df[\"grC\"].equals(df[\"grC.1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dict[1].keys())\n",
    "n_without_gaps = 0\n",
    "without_gaps = {}\n",
    "for n in df_dict:\n",
    "    df = df_dict[n]\n",
    "    df1 = (\n",
    "        df.sort_values(\"timestamp\")\n",
    "        .apply(lambda x: x.diff().max())\n",
    "        .reset_index(name=\"max_diff\")\n",
    "    )\n",
    "    display(df1)\n",
    "    if df1[\"max_diff\"][0].days < 2:\n",
    "        print(\"Without gap\")\n",
    "        n_without_gaps += 1\n",
    "        without_gaps[n] = df\n",
    "\n",
    "print(f\"Number of timeseries without gaps: {n_without_gaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_without_gaps = {}\n",
    "# Catchments without measurement gaps\n",
    "for id in sorted(without_gaps.keys()):\n",
    "    df = without_gaps[id]\n",
    "    # print(df)\n",
    "    _min = df[\"timestamp\"].min()\n",
    "    _max = df[\"timestamp\"].max()\n",
    "    _diff = _max - _min\n",
    "    print(f\"{id:3d} {_min} {_max} {_diff}\")\n",
    "    if _diff.days >= 5843:\n",
    "        long_without_gaps[id] = without_gaps[id]\n",
    "        # print('Added catchment')\n",
    "\n",
    "print(f\"\\nNumber of catchment with long measurement period: {len(long_without_gaps)}\")\n",
    "print(f\"Ids: {list(long_without_gaps.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_histogram(catchments):\n",
    "    # plot with various axes scales\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    all_nses = []\n",
    "    max_y = None\n",
    "    for offset in range(12):\n",
    "        plt.subplot(4, 4, offset + 1)\n",
    "        offset += 1\n",
    "        nses = []\n",
    "        all_nses.append(nses)\n",
    "        # Calculate nse based on model which predicts the previous day for all timeseries without gaps\n",
    "        for id in sorted(catchments.keys()):\n",
    "            runoff = catchments[id][\"m3/s\"].to_numpy()\n",
    "            offset = offset\n",
    "            nse = calc_nse(runoff[offset:], runoff[:-offset])\n",
    "            nses.append(nse)\n",
    "            # print(f'{id:02d}, nse: {nse:.3f}')\n",
    "\n",
    "        # the histogram of the data\n",
    "        n, bins, patches = plt.hist(\n",
    "            all_nses[-1],\n",
    "            bins=[0, 0.10, 0.20, 0.30, 0.40, 0.50, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        )\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(f\"{offset} days\")\n",
    "        if max_y is None:\n",
    "            max_y = max(n)\n",
    "        plt.ylim(top=max_y, bottom=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multi_histogram(without_gaps)\n",
    "plot_multi_histogram(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_and_validation_set(\n",
    "    df_dict, split_date, length, x_parameters=[\"mm\", \"grC\"], y_parameters=[\"m3/s\"]\n",
    "):\n",
    "    TRAIN_X_DICT = {}\n",
    "    TRAIN_Y_DICT = {}\n",
    "    TEST_X_DICT = {}\n",
    "    TEST_Y_DICT = {}\n",
    "    for i, _id in enumerate(df_dict):\n",
    "        # if i % 2 == 0:\n",
    "        #     print(f'{i+1}/{len(df_dict)}: Parsing catchment {_id}')\n",
    "        catchment_dataframe = df_dict[_id]\n",
    "        train_dataframe = catchment_dataframe[\n",
    "            catchment_dataframe[\"timestamp\"] <= split_date\n",
    "        ]\n",
    "        test_dataframe = catchment_dataframe[\n",
    "            catchment_dataframe[\"timestamp\"] > split_date\n",
    "        ]\n",
    "\n",
    "        TRAIN_X_DICT[_id] = substring_array(\n",
    "            train_dataframe[x_parameters].to_numpy()[:-1], length\n",
    "        )\n",
    "        TRAIN_Y_DICT[_id] = train_dataframe[y_parameters].to_numpy()[length:]\n",
    "\n",
    "        TEST_X_DICT[_id] = substring_array(\n",
    "            test_dataframe[x_parameters].to_numpy()[:-1], length\n",
    "        )\n",
    "        TEST_Y_DICT[_id] = test_dataframe[y_parameters].to_numpy()[length:]\n",
    "\n",
    "        assert len(TRAIN_X_DICT[_id]) == len(TRAIN_Y_DICT[_id])\n",
    "        assert len(TEST_X_DICT[_id]) == len(TEST_Y_DICT[_id])\n",
    "\n",
    "    return TRAIN_X_DICT, TRAIN_Y_DICT, TEST_X_DICT, TEST_Y_DICT\n",
    "\n",
    "\n",
    "def substring_array(array, length):\n",
    "    python_list = array.tolist()\n",
    "    new_list = []\n",
    "    for i in range(len(python_list) - length + 1):\n",
    "        new_list.append(python_list[i : i + length])\n",
    "    return np.array(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "TRAIN_X_DICT, TRAIN_Y_DICT, TEST_X_DICT, TEST_Y_DICT = create_test_and_validation_set(\n",
    "    long_without_gaps,\n",
    "    datetime.datetime(2007, 8, 31),\n",
    "    1,\n",
    "    x_parameters=[\"mm\", \"grC\", \"m3/s\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training using k_nearest_neighbour\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "stats_dict = {\"desc\": []}\n",
    "lookback_days = [1, 2, 3, 7, 30, 100, 365]\n",
    "x_parameters_list = [[\"mm\", \"grC\"], [\"mm\", \"grC\", \"m3/s\"]]\n",
    "\n",
    "for lookback in lookback_days:\n",
    "    print(f\"Number of days lookback: {length}\")\n",
    "    for use_rainoff_for_x in [False, True]:\n",
    "        x_parameters = (\n",
    "            x_parameters_list[1] if use_rainoff_for_x else x_parameters_list[0]\n",
    "        )\n",
    "        (\n",
    "            TRAIN_X_DICT,\n",
    "            TRAIN_Y_DICT,\n",
    "            TEST_X_DICT,\n",
    "            TEST_Y_DICT,\n",
    "        ) = create_test_and_validation_set(\n",
    "            long_without_gaps,\n",
    "            datetime.datetime(2007, 8, 31),\n",
    "            lookback,\n",
    "            x_parameters=x_parameters,\n",
    "        )\n",
    "\n",
    "        for n_neighbors in [1, 3, 5, 7, 10]:\n",
    "            stats_dict[\"desc\"].append(\n",
    "                f\"lookback {lookback}, use_rainoff {use_rainoff_for_x}, n_neighbors {n_neighbors}\"\n",
    "            )\n",
    "            for _id in TRAIN_X_DICT.keys():\n",
    "                # print(_id)\n",
    "                train_x, train_y, test_x, test_y = (\n",
    "                    TRAIN_X_DICT[_id],\n",
    "                    TRAIN_Y_DICT[_id],\n",
    "                    TEST_X_DICT[_id],\n",
    "                    TEST_Y_DICT[_id],\n",
    "                )\n",
    "\n",
    "                model = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "                train_x_flatend = train_x.reshape(\n",
    "                    (train_x.shape[0], -1), order=\"F\"\n",
    "                )  # https://stackoverflow.com/a/37500847\n",
    "                # print(train_x_flatend[0:2])\n",
    "                # print(train_y[0])\n",
    "                model.fit(train_x_flatend, train_y)\n",
    "\n",
    "                test_x_flatend = test_x.reshape((test_x.shape[0], -1), order=\"F\")\n",
    "                pred_y = model.predict(test_x_flatend)\n",
    "                nse = calc_nse(pred_y, test_y)\n",
    "                print(f\"Catchment {_id:2d}: {nse:.3f}\")\n",
    "                if _id not in stats_dict:\n",
    "                    stats_dict[_id] = []\n",
    "                stats_dict[_id].append(nse)\n",
    "\n",
    "display(nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(data=stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.to_excel(\"knn.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('tdt4195': conda)",
   "language": "python",
   "name": "python36964bittdt4195condad5a811ce7bba4261811e323200c936e0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
